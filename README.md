# NCTU_DLP_Lab01
No use library to implement  backpropagation neural network(train XOR and  linear data)

This is the first Lab of DLP to use backpropagation (back propagation) to complete the training of the basic neural network.
The program will have two inputs and one output. 

The principle of backpropation is to calculate the gradient descent method , we use XOR data and Linear data to train respeactively.

If you don't use backpropation to calculate, every time you calculate the gradient, you must continue to differentiate recursively.It will cause repeated operations.

Use the backprogation operation to record the backword items , In the program implementation, it will be better to write circle and easy to read.
